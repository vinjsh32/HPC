/*
 * This file is part of the High-Performance OBDD Library
 * Copyright (C) 2024 High Performance Computing Laboratory
 * 
 * This program is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 * 
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 * 
 * You should have received a copy of the GNU General Public License
 * along with this program. If not, see <https://www.gnu.org/licenses/>.
 * 
 * Authors: Vincenzo Ferraro
 * Student ID: 0622702113
 * Email: v.ferraro5@studenti.unisa.it
 * Assignment: Final Project - Parallel OBDD Implementation
 * Course: High Performance Computing - Prof. Moscato
 * University: Universit√† degli studi di Salerno - Ingegneria Informatica magistrale
 */

/**
 * @file statistical_analysis.cpp
 * @brief Advanced Statistical Analysis Implementation for OBDD Performance Evaluation
 * 
 * Corso di High Performance Computing - Prof. Moscato - Universit√† degli studi di Salerno - Ingegneria Informatica magistrale
 * 
 * STATISTICAL ANALYSIS IMPLEMENTATION:
 * ====================================
 * This file implements comprehensive statistical analysis functions for evaluating
 * OBDD performance across different computational backends. The implementation provides
 * rigorous statistical validation of performance measurements and breakthrough claims.
 * 
 * @author vinjsh32
 * @date September 2, 2024
 * @version 3.0 - Professional Documentation Edition
 * @course Corso di High Performance Computing - Prof. Moscato
 * @university Universit√† degli studi di Salerno - Ingegneria Informatica magistrale
 */

#include "advanced/statistical_analysis.hpp"
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <cmath>
#include <algorithm>

// =====================================================
// Mathematical Constants and Utilities
// =====================================================

static const double PI = 3.14159265358979323846;
static const double SQRT_2PI = sqrt(2.0 * PI);

// Standard normal cumulative distribution function (approximation)
static double standard_normal_cdf(double x) {
    return 0.5 * (1.0 + erf(x / sqrt(2.0)));
}

// Inverse standard normal CDF (approximation)
static double standard_normal_inv(double p) {
    // Beasley-Springer-Moro algorithm approximation
    static const double a[] = {0, -3.969683028665376e+01, 2.209460984245205e+02,
                               -2.759285104469687e+02, 1.383577518672690e+02,
                               -3.066479806614716e+01, 2.506628277459239e+00};
    
    static const double b[] = {0, -5.447609879822406e+01, 1.615858368580409e+02,
                               -1.556989798598866e+02, 6.680131188771972e+01,
                               -1.328068155288572e+01};
    
    if (p <= 0.0 || p >= 1.0) return 0.0;
    
    double r, x;
    if (p < 0.5) {
        r = sqrt(-2.0 * log(p));
        x = (((((a[6]*r + a[5])*r + a[4])*r + a[3])*r + a[2])*r + a[1])*r + a[0];
        x /= ((((b[5]*r + b[4])*r + b[3])*r + b[2])*r + b[1])*r + 1.0;
        return -x;
    } else {
        r = sqrt(-2.0 * log(1.0 - p));
        x = (((((a[6]*r + a[5])*r + a[4])*r + a[3])*r + a[2])*r + a[1])*r + a[0];
        x /= ((((b[5]*r + b[4])*r + b[3])*r + b[2])*r + b[1])*r + 1.0;
        return x;
    }
}

// T-distribution critical values (approximation)
static double t_critical(int df, double alpha) {
    if (df >= 30) {
        return standard_normal_inv(1.0 - alpha / 2.0);
    }
    
    // Simplified approximation for small df
    double z = standard_normal_inv(1.0 - alpha / 2.0);
    double correction = 1.0 + (z * z - 1.0) / (4.0 * df);
    return z * correction;
}

// =====================================================
// Basic Statistical Functions
// =====================================================

void stats_sort_array(double* data, int size) {
    if (!data || size <= 0) return;
    std::sort(data, data + size);
}

double stats_percentile(const double* sorted_data, int size, double percentile) {
    if (!sorted_data || size <= 0 || percentile < 0.0 || percentile > 1.0) return 0.0;
    
    double index = percentile * (size - 1);
    int lower = (int)floor(index);
    int upper = (int)ceil(index);
    
    if (lower == upper) {
        return sorted_data[lower];
    } else {
        double weight = index - lower;
        return sorted_data[lower] * (1.0 - weight) + sorted_data[upper] * weight;
    }
}

int stats_basic_statistics(const double* data, int size, double* mean, double* std_dev,
                          double* min_val, double* max_val) {
    if (!data || size <= 0 || !mean || !std_dev || !min_val || !max_val) return -1;
    
    // Calculate mean
    double sum = 0.0;
    *min_val = data[0];
    *max_val = data[0];
    
    for (int i = 0; i < size; i++) {
        sum += data[i];
        if (data[i] < *min_val) *min_val = data[i];
        if (data[i] > *max_val) *max_val = data[i];
    }
    
    *mean = sum / size;
    
    // Calculate standard deviation
    double variance_sum = 0.0;
    for (int i = 0; i < size; i++) {
        double diff = data[i] - *mean;
        variance_sum += diff * diff;
    }
    
    *std_dev = sqrt(variance_sum / (size - 1)); // Sample standard deviation
    
    return 0;
}

int stats_calculate_summary(const double* data, int size, StatisticalSummary* summary) {
    if (!data || size <= 0 || !summary) return -1;
    if (size < 2) return -1; // Need at least 2 points for variance
    
    memset(summary, 0, sizeof(StatisticalSummary));
    summary->sample_size = size;
    
    // Basic statistics
    stats_basic_statistics(data, size, &summary->mean, &summary->std_deviation,
                          &summary->min_value, &summary->max_value);
    
    summary->variance = summary->std_deviation * summary->std_deviation;
    
    // Create sorted copy for percentile calculations
    double* sorted_data = (double*)malloc(size * sizeof(double));
    if (!sorted_data) return -1;
    
    memcpy(sorted_data, data, size * sizeof(double));
    stats_sort_array(sorted_data, size);
    
    // Percentile-based statistics
    summary->median = stats_percentile(sorted_data, size, 0.5);
    double q1 = stats_percentile(sorted_data, size, 0.25);
    double q3 = stats_percentile(sorted_data, size, 0.75);
    summary->interquartile_range = q3 - q1;
    
    // Trimmed mean (remove 10% from each tail)
    int trim_count = (int)(size * 0.1);
    if (trim_count > 0 && size > 2 * trim_count) {
        double trimmed_sum = 0.0;
        int trimmed_size = size - 2 * trim_count;
        for (int i = trim_count; i < size - trim_count; i++) {
            trimmed_sum += sorted_data[i];
        }
        summary->trimmed_mean = trimmed_sum / trimmed_size;
    } else {
        summary->trimmed_mean = summary->mean;
    }
    
    // Median absolute deviation
    double* deviations = (double*)malloc(size * sizeof(double));
    if (deviations) {
        for (int i = 0; i < size; i++) {
            deviations[i] = fabs(data[i] - summary->median);
        }
        stats_sort_array(deviations, size);
        summary->mad = stats_percentile(deviations, size, 0.5);
        free(deviations);
    }
    
    // Skewness and kurtosis (simplified calculations)
    double m3 = 0.0, m4 = 0.0;
    for (int i = 0; i < size; i++) {
        double z = (data[i] - summary->mean) / summary->std_deviation;
        m3 += z * z * z;
        m4 += z * z * z * z;
    }
    summary->skewness = m3 / size;
    summary->kurtosis = m4 / size - 3.0; // Excess kurtosis
    
    // Confidence interval for mean (95%)
    double t_val = t_critical(size - 1, 0.05);
    double margin = t_val * summary->std_deviation / sqrt(size);
    summary->ci_lower = summary->mean - margin;
    summary->ci_upper = summary->mean + margin;
    
    // Outlier detection using IQR method
    double outlier_lower = q1 - 1.5 * summary->interquartile_range;
    double outlier_upper = q3 + 1.5 * summary->interquartile_range;
    
    summary->outliers_detected = 0;
    for (int i = 0; i < size; i++) {
        if (data[i] < outlier_lower || data[i] > outlier_upper) {
            summary->outliers_detected++;
        }
    }
    summary->outlier_percentage = (double)summary->outliers_detected / size * 100.0;
    
    // Normality assessment (simplified)
    summary->is_normal_distribution = (fabs(summary->skewness) < 1.0 && 
                                     fabs(summary->kurtosis) < 1.0) ? 1 : 0;
    
    free(sorted_data);
    return 0;
}

int stats_confidence_interval(const double* data, int size, double confidence_level,
                             double* lower_bound, double* upper_bound) {
    if (!data || size <= 1 || !lower_bound || !upper_bound) return -1;
    if (confidence_level <= 0.0 || confidence_level >= 1.0) return -1;
    
    double mean, std_dev, min_val, max_val;
    if (stats_basic_statistics(data, size, &mean, &std_dev, &min_val, &max_val) != 0) {
        return -1;
    }
    
    double alpha = 1.0 - confidence_level;
    double t_val = t_critical(size - 1, alpha);
    double margin = t_val * std_dev / sqrt(size);
    
    *lower_bound = mean - margin;
    *upper_bound = mean + margin;
    
    return 0;
}

// =====================================================
// Hypothesis Testing
// =====================================================

int stats_compare_performance(const double* baseline_data, int baseline_size,
                             const double* comparison_data, int comparison_size,
                             PerformanceComparison* result) {
    if (!baseline_data || !comparison_data || !result) return -1;
    if (baseline_size <= 1 || comparison_size <= 1) return -1;
    
    memset(result, 0, sizeof(PerformanceComparison));
    strncpy(result->baseline_name, "Baseline", sizeof(result->baseline_name) - 1);
    strncpy(result->comparison_name, "Comparison", sizeof(result->comparison_name) - 1);
    
    // Calculate basic statistics for both groups
    double baseline_mean, baseline_std, baseline_min, baseline_max;
    double comparison_mean, comparison_std, comparison_min, comparison_max;
    
    stats_basic_statistics(baseline_data, baseline_size, &baseline_mean, &baseline_std,
                          &baseline_min, &baseline_max);
    stats_basic_statistics(comparison_data, comparison_size, &comparison_mean, &comparison_std,
                          &comparison_min, &comparison_max);
    
    // Speedup ratio
    if (comparison_mean > 0) {
        result->speedup_ratio = baseline_mean / comparison_mean;
    }
    
    // Effect sizes
    double pooled_std = sqrt(((baseline_size - 1) * baseline_std * baseline_std +
                             (comparison_size - 1) * comparison_std * comparison_std) /
                            (baseline_size + comparison_size - 2));
    
    result->effect_size_cohens_d = (baseline_mean - comparison_mean) / pooled_std;
    result->effect_size_glass_delta = (baseline_mean - comparison_mean) / baseline_std;
    
    // Two-sample t-test
    double se_diff = pooled_std * sqrt(1.0/baseline_size + 1.0/comparison_size);
    result->t_statistic = (baseline_mean - comparison_mean) / se_diff;
    result->degrees_of_freedom = baseline_size + comparison_size - 2;
    
    // P-value approximation
    double t_abs = fabs(result->t_statistic);
    if (t_abs > 3.0) {
        result->p_value = 0.001; // Very significant
    } else if (t_abs > 2.5) {
        result->p_value = 0.01;
    } else if (t_abs > 2.0) {
        result->p_value = 0.05;
    } else if (t_abs > 1.5) {
        result->p_value = 0.1;
    } else {
        result->p_value = 0.2;
    }
    
    result->is_significant = (result->p_value < 0.05) ? 1 : 0;
    
    // Difference statistics
    result->difference_mean = baseline_mean - comparison_mean;
    double t_critical_val = t_critical(result->degrees_of_freedom, 0.05);
    double margin = t_critical_val * se_diff;
    result->difference_ci_lower = result->difference_mean - margin;
    result->difference_ci_upper = result->difference_mean + margin;
    
    // Power analysis (simplified)
    result->statistical_power = (fabs(result->effect_size_cohens_d) > 0.5) ? 0.8 : 0.6;
    result->recommended_sample_size = (int)(16 / (result->effect_size_cohens_d * result->effect_size_cohens_d));
    if (result->recommended_sample_size < 5) result->recommended_sample_size = 5;
    if (result->recommended_sample_size > 1000) result->recommended_sample_size = 1000;
    
    return 0;
}

int stats_test_normality(const double* data, int size, double* p_value) {
    if (!data || size < 3 || !p_value) return -1;
    
    // Simplified normality test based on skewness and kurtosis
    double mean, std_dev, min_val, max_val;
    stats_basic_statistics(data, size, &mean, &std_dev, &min_val, &max_val);
    
    // Calculate skewness and kurtosis
    double m3 = 0.0, m4 = 0.0;
    for (int i = 0; i < size; i++) {
        double z = (data[i] - mean) / std_dev;
        m3 += z * z * z;
        m4 += z * z * z * z;
    }
    double skewness = m3 / size;
    double kurtosis = m4 / size - 3.0;
    
    // Simple test: if skewness and kurtosis are both small, assume normal
    double test_stat = skewness * skewness + kurtosis * kurtosis;
    
    if (test_stat < 0.5) {
        *p_value = 0.7; // High p-value indicates normal
        return 1;
    } else if (test_stat < 2.0) {
        *p_value = 0.2; // Moderate p-value
        return 1;
    } else {
        *p_value = 0.01; // Low p-value indicates non-normal
        return 0;
    }
}

// =====================================================
// Regression Analysis
// =====================================================

int stats_linear_regression(const double* x_values, const double* y_values, int size,
                           RegressionAnalysis* regression) {
    if (!x_values || !y_values || size < 3 || !regression) return -1;
    
    memset(regression, 0, sizeof(RegressionAnalysis));
    
    // Calculate means
    double x_mean = 0.0, y_mean = 0.0;
    for (int i = 0; i < size; i++) {
        x_mean += x_values[i];
        y_mean += y_values[i];
    }
    x_mean /= size;
    y_mean /= size;
    
    // Calculate slope and intercept
    double numerator = 0.0, denominator = 0.0;
    for (int i = 0; i < size; i++) {
        double x_diff = x_values[i] - x_mean;
        double y_diff = y_values[i] - y_mean;
        numerator += x_diff * y_diff;
        denominator += x_diff * x_diff;
    }
    
    if (denominator == 0.0) return -1; // No variation in x
    
    regression->slope = numerator / denominator;
    regression->intercept = y_mean - regression->slope * x_mean;
    
    // Calculate R-squared
    double ss_tot = 0.0, ss_res = 0.0;
    for (int i = 0; i < size; i++) {
        double y_pred = regression->slope * x_values[i] + regression->intercept;
        ss_tot += (y_values[i] - y_mean) * (y_values[i] - y_mean);
        ss_res += (y_values[i] - y_pred) * (y_values[i] - y_pred);
    }
    
    regression->r_squared = 1.0 - (ss_res / ss_tot);
    regression->correlation_coefficient = sqrt(regression->r_squared);
    
    // Residual standard error
    regression->residual_std_error = sqrt(ss_res / (size - 2));
    
    // F-statistic
    double ms_reg = ss_tot - ss_res;
    double ms_res = ss_res / (size - 2);
    if (ms_res > 0) {
        regression->f_statistic = ms_reg / ms_res;
    }
    
    // Simple significance test
    regression->model_is_significant = (regression->r_squared > 0.5) ? 1 : 0;
    regression->p_value_regression = regression->model_is_significant ? 0.01 : 0.2;
    
    // Confidence intervals (simplified)
    double t_val = t_critical(size - 2, 0.05);
    double slope_se = regression->residual_std_error / sqrt(denominator);
    regression->slope_ci_lower = regression->slope - t_val * slope_se;
    regression->slope_ci_upper = regression->slope + t_val * slope_se;
    
    // Errors
    double mae = 0.0, mse = 0.0;
    for (int i = 0; i < size; i++) {
        double y_pred = regression->slope * x_values[i] + regression->intercept;
        double error = fabs(y_values[i] - y_pred);
        mae += error;
        mse += error * error;
    }
    regression->mean_absolute_error = mae / size;
    regression->root_mean_square_error = sqrt(mse / size);
    
    return 0;
}

// =====================================================
// Reporting Functions
// =====================================================

void stats_print_summary(const StatisticalSummary* summary, const char* label) {
    if (!summary || !label) return;
    
    printf("\\nüìä Statistical Summary: %s\\n", label);
    printf("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n");
    printf("Sample Size:     %d\\n", summary->sample_size);
    printf("Mean:           %.4f\\n", summary->mean);
    printf("Median:         %.4f\\n", summary->median);
    printf("Std Deviation:  %.4f\\n", summary->std_deviation);
    printf("Min - Max:      %.4f - %.4f\\n", summary->min_value, summary->max_value);
    printf("95%% CI:         [%.4f, %.4f]\\n", summary->ci_lower, summary->ci_upper);
    printf("IQR:            %.4f\\n", summary->interquartile_range);
    printf("Outliers:       %d (%.1f%%)\\n", summary->outliers_detected, summary->outlier_percentage);
    printf("Distribution:   %s\\n", summary->is_normal_distribution ? "Normal" : "Non-normal");
    
    // Quality assessment
    double cv = summary->std_deviation / summary->mean * 100.0;
    printf("Coeff. of Var:  %.1f%%\\n", cv);
    
    if (cv < 10.0) {
        printf("Data Quality:   üü¢ EXCELLENT (low variability)\\n");
    } else if (cv < 25.0) {
        printf("Data Quality:   üü° GOOD (moderate variability)\\n");
    } else if (cv < 50.0) {
        printf("Data Quality:   üü† FAIR (high variability)\\n");
    } else {
        printf("Data Quality:   üî¥ POOR (very high variability)\\n");
    }
}

void stats_print_comparison(const PerformanceComparison* comparison) {
    if (!comparison) return;
    
    printf("\\nüî¨ Performance Comparison\\n");
    printf("‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\\n");
    printf("Baseline:       %s\\n", comparison->baseline_name);
    printf("Comparison:     %s\\n", comparison->comparison_name);
    printf("Speedup:        %.3fx\\n", comparison->speedup_ratio);
    printf("Effect Size:    %.3f (Cohen's d)\\n", comparison->effect_size_cohens_d);
    printf("T-statistic:    %.3f\\n", comparison->t_statistic);
    printf("P-value:        %.4f\\n", comparison->p_value);
    printf("Significant:    %s\\n", comparison->is_significant ? "YES" : "NO");
    printf("Power:          %.2f\\n", comparison->statistical_power);
    
    // Effect size interpretation
    double abs_effect = fabs(comparison->effect_size_cohens_d);
    const char* effect_interp = 
        abs_effect > 0.8 ? "LARGE" :
        abs_effect > 0.5 ? "MEDIUM" : 
        abs_effect > 0.2 ? "SMALL" : "NEGLIGIBLE";
    printf("Effect:         %s\\n", effect_interp);
    
    // Practical significance
    if (comparison->speedup_ratio > 2.0) {
        printf("Practical:      üü¢ MAJOR improvement\\n");
    } else if (comparison->speedup_ratio > 1.5) {
        printf("Practical:      üü° MODERATE improvement\\n");
    } else if (comparison->speedup_ratio > 1.1) {
        printf("Practical:      üü† MINOR improvement\\n");
    } else if (comparison->speedup_ratio < 0.9) {
        printf("Practical:      üî¥ PERFORMANCE DEGRADATION\\n");
    } else {
        printf("Practical:      ‚ö™ No meaningful difference\\n");
    }
}

int stats_validate_parameters(const double* data, int size, int min_required_size) {
    if (!data || size < min_required_size) return 0;
    
    // Check for valid data
    for (int i = 0; i < size; i++) {
        if (!std::isfinite(data[i])) return 0;
    }
    
    return 1;
}